# 1.自我介绍

面试官您好，我叫李哲，来自山东济南，本科毕业于中国矿业大学计算机科学与技术专业，保研后继续在该学校该专业就读，本硕成绩优异，并多次校级奖学金。在第十六届蓝桥杯算法竞赛中获得江苏省一等奖。

科研方面，我以第一作者身份在人工智能领域国际期刊《Knowledge-Based Systems》（中科院一区TOP，CCF-C类）发表论文一篇，中文译名基于高置信度正负样本对选择的三重对比式图聚类网络，题为《Triple-View Graph Clustering Network Based on High-Confidence Contrastive Learning Strategy》，研究了多视角图对比学习在聚类任务中的应用。

2025年暑期，我在途牛旅游网担任后端开发实习生，参与建行定制游和移动疗休等核心业务开发，完成了用户体系对接、订单链路闭环、接口重构及横向越权漏洞修复等工作，积累了微服务架构下的工程实践经验。

技术上，我专注于 Java 后端开发，熟练掌握 Spring Boot、MyBatis 等主流框架，具备扎实的系统设计和接口开发能力。在高并发场景下有实战经验：曾独立开发“仿淘宝神券系统”，综合运用 Redis + Lua 实现库存原子扣减、RocketMQ 异步解耦、ShardingSphere 分库分表、布隆过滤器防穿透等技术，并通过设计模式和幂等机制提升系统可维护性与稳定性。

# 2.实习内容

## 中国移动疗休：

> 为支持移动疗休业务的功能扩展，升级订单同步接口

### 重构同步接口：

重构酒店、火车、飞机、用车四大核心品类的订单推送逻辑，完成移动新增字段的推送功能;

1. 业务背景：移动方面需要升级订单详情接口，升级后的订单详情需要补充大量的字段，我负责对接这部分内容
    
2. 主要工作：一个是根据移动提供的接口文档，梳理字段，去找缺少的字段从哪里可以获得，然后获取这些信息；另一个是重构推送接口所推送的内容，确保我们推送的信息能够符合移动方的需求
    
3. 工作1:移动缺少的是这个订单所属的品类的详细信息，一共有四大类：火车票；机票；酒店；用车，这四大类在我们公司所属于不同的系统，公司有一个整合的系统叫pga，这些下属的不同品类的详细信息可以通过pga完成透传，这样我们就能通过订单ID获取到一个json对象，再通过解析这个json对象就可以拿到不同品类的信息；
    
4. 接下来就是比较复杂的编码工作了，这部分难度不大但是整体结构十分繁杂，酒店要推送的对象比较简单，大部分信息都在非集合变量中，有关房间具体信息，住户具体信息的作为集合变量；火车票需要将整个订单中的所有票打包成集合（比如两个人从a到b到c，有四张票，这个订单集合就有四个元素），除此之外，改签和退票信息也是类似的集合形式；飞机票类似火车票；用车分成三个，分别是接送机，租车，包车
    
5. 最后是和移动进行联调上线
    

### 修复安全漏洞：

针对查看出团通知书接口，通过RPC调用用户服务进行身份校验，成功修复了横向越权安全漏洞，保障了用户数据的隐私与安全;

1. 首先介绍这个接口的背景：这是我实习的第一个需求，我接到的需求是根据用户的订单id返回这个用户的出团通知书，起初我认为这个工作比较简单，然后开始了接口文档和技术方案的撰写，入参是一个订单ID，出参一个统一封装的response，将String的URL作为message，进行了开发
    
2. 但是mentor检查是提出我没有注意到会出现的横向越权问题，这个问题是如果我只以订单ID做为参数，去查询，那么就会出现一种情况，另一个用户输入了错误的订单ID，那么他就能查到那个ID对应的出团通知书，所以，需要多加一步判断：于是我就加了一步判断：确认当前用户是订单对应的用户才往下获取URL，
    
3. 最后整体流程如下：获取订单ID，通过调用远程的下游服务获取到该订单的userID，并从登录态中拿到当前登录用户的id，判断一致后远程调用下游服务的接口获取该订单对应的URL
    

## 建行定制游相关

业务背景：该项目属于建行疗休，由于我们提供的产品没发满足建行用户的个性化需求，因此选择开发定制游

### 整合用户体系：

实现建行用户静默注册与身份绑定逻辑，通过外部ID与手机号，无缝对接第三方与公司内部会员体系;

定制产品打包与预定单下单：通过RPC依次调用动态打包产品、订单中心等多个下游服务，将建行员工提交的个性化需求（行程、城市、日期等）实时转化为系统内的产品，并生成预定订单；

建行定制游部分主要分三部分：2.18 定制游需求提交，2.19 供应商定制产品推送， 2.17 供应商额外收款

建行有自己的善融app，如果选择我们的话会走我们的定制游逻辑，根据2.18，首先，建行用户提交过来的都是在建行的信息，封装在一个dto中，用户来了，我们首先要为该用户进行静默注册，将建行用户转化为我们的用户后才能进行后续的操作

1. 静默注册：静默注册是在后台完成注册，不会显示的打扰到用户，提高用户的使用体验，静默注册这部分我们主要做了：首先构建一个绑定方法的参数请求dto,这个dto中主要需要建行的userId和手机号，以及将建行对应的伙伴码和P值（写死），将他们写入这个dto中；接着我们用这个请求去远程调用绑定服务，这个服务是专门为第三方用户注册的；绑定成功后，系统后台就有了这个建行用户对应的途牛ID了，这时候我们就可通过建行ID查到途牛ID，这一步依然是通过掉远程接口拿到的，至此，我们就将这个建行用户转化为了途牛用户，并且拿到了ID
    
2. 动态产品打包：动态打包产品，也就是根据用户的自定义需求，实时生成一个动态的产品，首先跟上面一样，构建出一个动态产品的dto，接着，调用远程接口，将这个产品做为参数传入，远程接口返回这个动态产品的动态ID，接着我们构建一个输出的dto，包含产品名称，城市信息，出行信息等内容，这个输出的dto就做为我们获取到的动态产品。
    
3. 调用pga下单：下单部分需要前两步获取的用户ID和产品信息，利用他们构建下单的参数，这个请求中需要包含订单配置信息（用户id，来源类型），备注信息（备注主要是解析产品并转化成备注信息，需求号多少？出发地目的地等），设置下单系统信息，设置渠道信息（建行的渠道相关内容，p值等），最后是子订单信息，子订单可以视作我们习惯上看到的订单；构建好订单信息后调用下单接口完成下单返回结果，此时我们就拿到了这个定制游的订单id，接着在需求表中记录这条需求，这样这个接口的任务就完成了
    

### 业务数据持久化：

设计建行员工出游需求表以持久化关键数据，通过引入version字段实现乐观锁，有效保证了需求单状态更新的一致性，确保了链路的可追溯性；并开发了产品信息回推接口，将针对用户定制的产品同步至建行，最终完成了与第三方渠道的完整业务闭环；

产品回推接口即2.19 供应商定制产品推送：这部分的场景为：我们的客服跟单完成后，我们需要将完成的信息落库，并且将产品信息推送给建行：

1. 入参数为订单ID，客服ID，用户ID，首先我们需要根据订单ID拿到需求单ID，更新需求单ID的客服部分
    
2. 根据需求单的内容，构建一个建行产品，并且写入我们系统的产品库（区分之前的动态产品库）
    
3. 根据建行的文档，构建符合文档要求的output对象，并且通过远程调用的形式推送给建行


## 问：实习的困难和挑战
1. 逐步适应真实的开发流程：在学校，我可能写完代码、功能通过就结束了。但在实习中，我必须学习一套完整的流程：从接收需求、撰写技术方案、参加方案评审、编码实现，到最后的联调上线。
2. 需要考虑到实际场景深入分析业务：还是以那个安全漏洞为例，我最初的分析是“输入一个订单ID，返回一个URL”，这是一个纯技术视角。在被指出“横向越权”风险后
3. 如何与性格不同的人进行团队合作：多问多沟通，不要一个人闷着头做


## 问：这个 `pga` 系统具体扮演了什么角色？它是一个API网关，还是一个服务聚合层？
`pga` 在这里的角色更像是一个**服务聚合层（Service Aggregation Layer）**。它封装了对下游多个垂直业务系统（如酒店、火车票系统）的调用细节。我们上游的业务服务（疗休服务）不需要知道下游各个系统的具体接口地址或协议，我们只需要按照`pga`定义的规范，传入一个订单ID，`pga` 就会去“透传”这个请求，调用对应的下游系统，拿到详情，然后返回一个聚合后的JSON对象。

**为什么不让前端传`userId`？** 绝对不行。安全设计的核心原则是**“永不信任客户端”**。如果前端同时传递 `orderId` 和 `userId`，攻击者可以轻易地伪造请求，用自己的 `userId`（A）去匹配别人的 `orderId`（B）。这种校验毫无意义。

## 问：新增的RPC调用（`orderId` -> `userId`）是否会对接口性能产生明显影响？有没有考虑过其他实现方式，

**为什么后端RPC是最好的？**

1. **数据权威性：** 订单（`orderId`）和用户（`userId`）的归属关系，**唯一权威的来源**是订单服务。我的服务（疗休服务）必须向订单服务查询，才能确认“这个`orderId`到底是不是属于你这个登录用户”。
    
2. **职责分离：** 当前登录用户的`userId`从登录态（如Session或Token）中获取，这是可信的。`orderId`从请求参数中获取，这是不可信的。用可信的数据（登录态`userId`）去校验不可信数据（`orderId`）的归属权，这个逻辑必须在后端完成。
    

## 问：服务编排使用分布式事务相关内容

这个流程涉及多个外部RPC调用和本地DB操作，是一个典型的**分布式事务**场景。

在当前的代码实现中，我们并没有采用重量级的分布式事务方案（如Seata或2PC/3PC），因为这会显著增加系统复杂度和性能开销。我们的处理策略是**“最终一致性 + 补偿机制”**。

1. **代码中的处理：** 如你所见，代码`handleNewDemandWithOrder`中使用的是`try-catch`块。如果在`createPgaOrder`成功后，`saveOrUpdateTourDemandRecord`失败了，`catch`块会捕获异常，记录详细的错误日志（包含`demandId`、`orderId`等关键信息），并向上层返回一个明确的失败响应。
    
2. **补偿机制（核心）：**
    
    - **人工/自动补偿（上游）**：我们依赖`try-catch`中捕获的**详细错误日志**。这些日志会接入公司的监控告警平台。一旦出现“PGA下单成功但本地保存需求失败”的特定错误，会触发告警，由SRE或开发人员介入，分析日志并手动触发补偿流程（比如拿日志中的`orderId`重新调用一次保存接口）。
        

## 乐观锁

# 3.项目经历
## 问：项目中的难点与挑战是什么
主要在于学习和尝试使用新的中间件来优化现有的内容，尝试将设计模式的思想融入开发中：“仿淘宝神券系统” 中，要支撑大量用户同时秒杀领券，必须解决高并发下的性能、数据一致性以及代码可扩展性的问题。

在寻求改进的过程中，学习和利用到了很多之前没有接触过的技术和设计模式。最终也是基于 **Redisson**、**Lua 脚本**、**RabbitMQ**（或 RocketMQ）等中间件，以及**责任链**和**模板方法**等设计模式，完成了一个整体的优化。

具体来说：

- **中间件应用：**
    
    - 用 **Redisson** 的分布式锁来解决幂等性问题（它解决了可重入、可重试、超时释放、主从一致性四个问题）。
        
    - 用 **Lua 脚本**来封装 Redis 中的“判断库存”和“扣减库存”操作，解决这组条件判断操作的原子性，防止超卖。
        
    - 用**RocketMQ**执行异步数据库操作，将同步写库操作解耦，以此来削峰填谷，极大提高了秒杀接口的性能。
        
- **设计模式应用：**
    
    - 应用**责任链设计模式**来分层校验优惠券模板参数，取代了复杂的 `if-else`，提升了系统的拓展性。
        
    - 运用**模板方法设计模式**重构了消息队列的发送功能，将固定的发送流程和易变的配置逻辑分离，增强了代码复用性。

## 问：设计模式中的责任链与模版
在优惠券模块开发中，我负责了“优惠券模板创建业务”的整体设计与实现。

这个模块的业务逻辑相对复杂，比如：不同类型的优惠券（平台券和店铺券）、优惠对象（商品专属或全店通用）、领取规则、消耗规则等都有较多的参数要求，导致参数校验逻辑变得非常冗长。

一开始我是在 `createCouponTemplate` 接口中手动写了大量 if 判断，对请求参数做各种非空和业务合法性校验。但后期维护成本非常高，比如一旦新增一个校验逻辑，就容易改动一大坨代码，还容易出错。

为此我对这块逻辑做了架构优化 —— **采用责任链模式** 把所有校验拆成独立模块，每个校验逻辑封装成一个处理器，实现了高度解耦、职责单一。

我定义了一个统一的责任链接口 `MerchantAdminAbstractChainHandler<T>`，每个处理器都实现它，并注册进 Spring 容器。责任链上下文 `MerchantAdminChainContext` 会在启动时收集所有实现类，并根据业务标识（如：优惠券模板创建）进行排序和调度。这样在创建优惠券时，只需要调用：

```
merchantAdminChainContext.handler(MERCHANT_ADMIN_CREATE_COUPON_TEMPLATE_KEY, requestParam);
```

框架就会自动按照顺序去执行每个校验逻辑（比如非空校验、数据格式校验、商品合法性校验等），而不用手动管理调用流程。
### 为什么用责任链，有什么优势

因为优惠券模板创建的参数验证逻辑非常多，比如非空、格式合法性、库存是否为正、商品是否存在等等，后期还可能新增新的校验规则。

传统方式是把所有逻辑写在一个方法里，非常臃肿，违反了“单一职责原则”。而责任链模式可以将每一个校验逻辑抽象成一个处理器，职责明确、互不影响，新增或移除某个校验只需操作一个类，符合“开闭原则”。

并且我通过 Spring 容器自动收集、排序这些处理器，避免了硬编码，增强了通用性和可扩展性。

### 如果某个处理器执行失败了，怎么处理？责任链是继续往下走还是中断？

我设计的时候是根据不同校验的严重程度决定：

- 如果是**强校验**（比如缺少必须参数），会直接抛出异常，中断后续处理；
- 如果是**可容忍的校验**（比如字段格式不规范但不影响业务逻辑），可以记录日志并继续往下走。

这个机制是在每个处理器中通过异常机制控制的。责任链上下文只是负责顺序执行，并不会强制处理异常，由各处理器自行决定是否抛出错误。

### 你是怎么保证责任链执行顺序的？如果顺序错了会不会影响业务？

我让每个处理器实现了 Spring 的 `Ordered` 接口，通过返回 `getOrder()` 值来指定执行优先级。系统启动时我在 `MerchantAdminChainContext.run()` 方法中，对每一组责任链按顺序进行排序。

比如：非空校验是最基础的，它优先级最低（值最小），会最早执行；像商品是否存在这类依赖性强的逻辑，会排在后面执行。

这确保了处理流程的正确性，也避免了因为顺序错误导致依赖数据为空的异常。

### 如果后续要新增一个新的校验逻辑，比如优惠券时间不能小于当前时间，该怎么做？

只需要新增一个实现了 `MerchantAdminAbstractChainHandler<T>` 的校验处理器类，并设置好其 `mark()` 与 `getOrder()` 方法：

**这个模版类主要有三个方法：**

1.抽象的构建发送消息的配置信息的方法：buildBaseSendExtendParam

2.抽象的构建所要发送给mq的消息整体的方法：buildMessage

3.实际上生效的固定的发送方法，实现这抽象类的子类都是用这个方法完成发送的

在这个发送流程中，一个消息是通过一个发送事件sendMessage来传递的，那么第一个抽象方法就以这个信息为参数，解析构建这条信息的topic key等mq需要的配置信息

第二个抽象方法以第一个方法的返回结果和sendmessage为参数，构建获得最后需要发送给mq的对象

第三个方法可以发送第二个中获得的消息信息，这个消息sendmessage根据不同业务有不同的种类，所以我们使用了一层wrapper来包装一下，以后消费者同意接受这个为参数即可

通过这个模式，我把“如何构建配置”和“如何构建消息体”这两个**易变**的步骤开放给子类，而把“发送-日志”这个**不变**的流程固化在父类，实现了代码的高度复用和解耦。

## 问：为什么选择“线程池 + Redisson延迟队列”这个组合，而不是一个更统一的方案？

Controller发MQ，Consumer做解析”是这个场景的**标准解**。它职责清晰，并且能利用MQ的可靠投递、重试和死信队列来保证任务“至少执行一次”，这比我自己实现的`ThreadLocal` + `Redisson`兜底要健壮得多。

当时没有采用这个方案，主要是出于两点考虑：

1. **技术栈探索：** 正如文档里提到的，我们想尝试`Redisson`延迟队列的用法，探索在没有`RocketMQ`（或不想为这种“非核心”业务引入重量级MQ）的场景下，如何用Redis实现一个轻量级的异步任务兜底。
    
2. **业务场景：** 这个“Excel分发”在我们的设定中是一个**后台运营功能**，并发量很低，而且对实时性要求不高（5秒或30秒完成都可以接受）。这使得我们有空间去“牺牲”一点健壮性来尝试不同的技术实现。
    

## 问：你提到`t_coupon_template`表使用`shop_number`作为分片键，并且在实践中发现，默认的`HASH_MOD`算法导致了数据倾斜，具体表现为“0 库的奇数表没有数据，1 库的偶数表没有值”。

**倾斜原因：** 店铺ID是我们系统中的一个自增ID。 标准的`HASH_MOD`算法， 假设我们分2个库（`sharding-count: 2`），分表算法也是对`shop_number`取模（比如模8）。如果`shop_number`**全是偶数**，那么它们对2取模（分库）会**全部落在 `ds_0`**；它们对8取模（分表）会**全部落在 `t_coupon_template_0`, `_2`, `_4`, `_6`**。 这就是文档中“0 库的奇数表没有数据，1 库的偶数表没有值”（或者反过来）这个现象的根本原因：**分片键的原始分布**与**分片算法（取模）**之间产生了“共振”。

**自定义算法如何解决：** 我们的`CLASS_BASED`算法，核心就在于`hashShardingValue(id)`这个方法。这个方法不能直接使用`id`本身，而是要对`id`进行一次“扰动”计算，使其结果的分布更均匀。 我们参考了`Java`中`HashMap`的扰动函数（hashing function）。`HashMap`为了防止哈希冲突，会将`key`的`hashCode()`**高16位**和**低16位**进行**异或（`^`）操作**。

Java

static final int hash(Object key) {  
    int h;  
    return (key == null) ? 0 : (h = key.hashCode()) ^ (h >>> 16);  
}

我们的`hashShardingValue(id)`也采用了类似的原理：

Java

// 这是一个示例  
private long hashShardingValue(long id) {  
    // 将高32位和低32位进行异或  
    return id ^ (id >>> 32);  
}

这样做的好处是，即使原始的`shop_number`（`id`）在低位上（比如奇偶性）分布不均，但通过**引入高位的信息（`id >>> 32`）进行异或**，可以使得最终结果的低位（将被用于取模）变得**接近均匀分布**。这就打断了原始数据分布和取模算法之间的共振，从而将数据均匀地“撒”到所有库和所有表中。


## 我看你用到了二级缓存，介绍一下

> 这部分去背优化[【黑马点评优化】之使用Caffeine+Redis实现应用级二层缓存_caffeine redis二级缓存-CSDN博客](https://blog.csdn.net/m0_52031708/article/details/142862864?spm=1001.2014.3001.5502)

如果只使用redis来做缓存我们会有大量的请求到redis，但是每次请求的数据都是一样的，假如这一部分数据就放在应用服务器本地，那么就省去了请求redis的网络开销，请求速度就会快很多； 如果只使用Caffeine来做本地缓存，我们的应用服务器的内存是有限，并且单独为了缓存去扩展应用服务器是非常不划算。所以，只使用本地缓存也是有很大局限性的； 因此在项目中，我们可以将热点数据放本地缓存，作为一级缓存，将非热点数据放redis缓存，作为二级缓存，减少Redis的查询压力。

我在项目中采用了以下二级缓存策略：

- **一级缓存**使用 Caffeine，这是一个高性能的本地缓存库，运行在应用服务的 JVM 内存中，适合缓存热点数据，访问速度极快（纳秒级）并支持 LRU 淘汰策略；
    
- **二级缓存**是 Redis，负责缓存大部分业务数据，适合跨服务共享；
    
- 缓存读取逻辑为：**先查本地 → 本地未命中查 Redis → Redis 未命中再查数据库 → 数据库查询结果回写 Redis 与本地缓存**；
    
- 为避免缓存穿透与击穿问题，配合使用**布隆过滤器**防止查询不存在数据、**逻辑过期机制**避免瞬时并发击穿缓存层。
    

这种设计显著降低了延迟与 Redis 压力，特别是在高 QPS 场景下能有效防止 Redis 被击穿，提升整体系统可用性。

## 怎么保持缓存一致性的

一开始我们只使用了redis来做缓存，使用cas(cache a)方法来保持缓存一致性，具体操作是先更新数据库再删缓存，如果先删除缓存，那么如果这是个高频访问的key就会造成一个类似于缓存击穿的现象，先删数据库还有小概率会出现脏数据的现象（线程1查询未命中更新缓存，还没来得及写，线程2插入更新了数据，就造成了脏数据），其实还可以使用延迟双删来解决这个问题：先删除缓存再更新数据库，延时一段时间后再删缓存

引入 Caffeine 之后，需要处理 Redis 与 Caffeine 的一致性问题，为此我集成了 **阿里巴巴 Canal 中间件**，监听数据库的 binlog 日志；

- Canal 伪装为 MySQL 从库监听 binlog；
    
- 通过C_hange Data Cap_ture机制，当数据库数据发生变更时，会通知服务端监听器；
    
- 监听器根据变更类型刷新本地与 Redis 中的缓存，实现三者之间的数据同步。
    

## 看你用到了布隆过滤器，你是怎么使用的，它的参数（数组长度，哈希函数的个数）要如何设置？会误判吗？

为了防止缓存穿透，我使用了 **布隆过滤器（Bloom Filter）** 来拦截大量请求中对数据库中不存在的数据访问。它实际上是一个很长的二进制向量和一系列随机映射函数。布隆过滤器可以用于检索一个元素是否在一个集合中。它的优点是空间效率和查询时间都比一般的算法要好的多，缺点是有一定的误识别率和删除困难。可以快速判断一个数据“可能存在”或“一定不存在”。

使用上，我采用的是 **Guava 提供的 BloomFilter** 类库。**预计插入数量（n）**：预计将要加入布隆过滤器的数据条数，我预估的是 1w；**误判率（f）**：控制误判概率，1% 是一个常用且折中选择；底层会自动根据这两个参数计算出最优的位数组长度（m）和哈希函数个数（k），我们不需要手动设定；

### 是否会误判？

会，但只会出现**“误判存在”**，不会出现**“误判不存在”**。这意味着：布隆过滤器可能说某个 key 存在，但其实它没有，但不会把一个真的 key 给错判为不存在。因此，我们配合 Redis 和 DB 再做一次兜底判断，整体上非常安全可靠。

## 你用雪花算法生成全局唯一id，为什么要使用全局唯一id，雪花算法的优缺点有哪些？

在分布式系统中，我们需要为业务数据生成**唯一且有序的ID**，比如用户ID、订单号等。使用数据库自增ID会有性能瓶颈和单点问题，因此我采用了**雪花算法（Snowflake）**来生成全局唯一ID。

### 使用雪花算法的好处：

- **高性能**：在内存中生成，吞吐量高；
    
- **全局唯一**：结合时间戳+机器标识+序列号，避免重复；
    
- **趋势递增**：便于按时间排序，提高写入性能；
    
- **不依赖数据库**：适用于分布式系统、微服务架构
    

### 缺点或风险：

- **依赖系统时间**：如果服务器时间回拨，可能会导致ID重复或系统阻塞；
    
- **ID较长**：占用long型（64位），在某些极端存储或传输场景中略大；
    
- **机器ID配置复杂**：需要保证不同服务实例间机器ID不重复，否则可能会冲突。
    

我在项目中使用自己封装的 snowflake 工具类实现 ID 生成，有效保障了系统的高并发写入场景下 ID 唯一性与有序性。

## 如何进行库存控制（防止超卖）？如何保持Redis和数据库数据一致？乐观锁原理？在CAS失败时如何处理？

### 库存控制流程（防止超卖）：

1. **Redis预扣减库存**：用户秒杀时，先在Redis中判断库存是否充足，此处我们使用cas乐观锁的原理，当目前库存大于零才允许下单；此处我们的逻辑都是在lua脚本中判断的，因为需要同时判断超卖和一人一单问题
    
2. **请求进入消息队列**：将扣减请求异步发送至RabbitMQ等消息队列，进行流量削峰；
    
3. **后台异步消费落库**：
    
    - 从队列中拉取请求，去数据库执行真正的扣减；
        
    - 使用**乐观锁**保证并发下数据库扣减正确性。
        

## 失败如何处理

失败说明库存为零，直接返回库存不足

## 为什么要用Lua防止超卖？Lua和Redission作用的关系是什么，替代还是互补？

lua脚本的工作是为了保障对redis的操作是原子性的，这里我们使用lua脚本来进行库存和一人一单的判断，是为了保证同时满足这两个条件才能进行下单。而redisson则是redis的常见java客户端，**是一个分布式协调工具包**，提供了很多高级功能，比如分布式锁、读写锁等。redisson中的一些功能是借助lua脚本实现的，他们在本项目中是互补的关系。

### Redisson 的优势：

- 封装简洁，使用成本低；
    
- 功能全面，涵盖多种分布式场景；
    
- 内置容错机制（自动续期、超时释放）；
    
- 支持多种 Redis 部署模式（单机、主从、哨兵、集群）。
    

## 如何使用redisson+mq异步下单的，描述一下过程

- 用户点击“抢购”按钮，前端发起请求；
    
- 后端调用 Lua 脚本进行 Redis 原子校验（库存、一人一单），成功后：
    
    - 使用 Snowflake 生成订单 ID；
        
    - 将 `voucherId`、`userId`、`orderId` 打包成消息写入 RabbitMQ；
        
- 消费端（RabbitMQConfig）异步监听队列消息：
    
    - 从消息中提取信息；
        
    - 获取 Redisson 分布式锁，防止重复下单；
        
    - 调用 `createVoucherOrder` 方法，将订单落库；
        
    - 成功后释放锁，消息确认；
        

## 使用了消息队列，可靠性怎么保证

在以下三个过程中会出现问题

过程1：生产者（voucherorderservice中的seckillvoucher方法）连接RabbitMQ消息队列 过程2：生产者（seckillvoucher）投递订单消息（VoucherOrder）到消息队列中 过程3：消费者（RabbitMQConfig中的recieve Messeage）从消息队列中取出消息（VoucherOrder）消费

1. 生产者可靠性使用的时生产者确认机制，消息从生产者发出到中间件接受，只要中间件接受到了就说明信息没有丢失，中间件是否接受可以用ack相应来确认，如果生产者收到了ack相应，说明消息成功传达
    
2. 消息队列做持久化，设置参数
    
3. 消费者确认：当消费者处理消息结束后，应该向RabbitMQ发送一个回执，告知RabbitMQ自己消息处理状态。回执有三种可选值：
    
    ack：成功处理消息，RabbitMQ从队列中删除该消息 nack：消息处理失败，RabbitMQ需要再次投递消息 reject：消息处理失败并拒绝该消息，RabbitMQ从队列中删除该消息 一般reject方式用的较少，除非是消息格式有问题，那就是开发问题了。因此大多数情况下我们需要将消息处理的代码通过try catch机制捕获，消息处理成功时返回ack，处理失败时返回nack.
    

## 消息队列业务幂等性问题

因为我们使用了全局唯一id，所以插入订单等情况天然就是幂等的，不过我们可以有一个做法来处理业务幂等性，就是用唯一消息id，每一条消息都声称一个唯一的id，与消息一起投递给消费者，消费者接受到消息后处理自己的业务，业务处理成功后将消息ID保存到数据库，如果下次又收到相同消息，去数据库查询判断是否存在，存在则为重复消息放弃处理

除此之外还可以根据具体的业务逻辑判断，比如我们修改支付状态的时候，首先检查一下是否是未支付，如果是的话才可以修改位已支付，相比较而言，使用消息id的话需要改造原有的数据库，在项目中使用业务逻辑更好

## 线程池

秒杀系统中，**线程池应用在了MQ消费者端，作为高并发订单异步处理的组件**：面对秒杀场景下的大规模流量，我引入了线程池来对订单消息进行动态落库，初始基于CPU核心数配置基础处理能力，当订单消息涌入时线程池自动扩容至预设上限高效消费，业务低谷期则回收空闲线程避免资源浪费；

CPU密集型：corePoolSize = CPU核数 + 1（避免过多线程竞争CPU）

IO密集型：corePoolSize = CPU核数 x 2（或更高，具体看IO等待时间）

秒杀业务特点瞬时高并发、任务处理时间短，因此设置

核心线程为cpu两倍，最大线程数为核心*两倍，空闲10秒钟回收，线程池不进行缓存直达线程，直接拒绝达成快速失败

一般不能excutors快捷创建，newFixedThreadPool 和 newCachedThreadPool，可能因为资源耗尽导致 OOM 问题，而是手动new一个threadpoolexcutor，根据实际情况手动配置

## 限流是怎么做的

算出1k用户即为性能瓶颈，超过1k用户后，吞吐量以及响应时间会飞速下降。因此选择使用限流来控制流量，因此sentinel后续限流的QPS也为1k