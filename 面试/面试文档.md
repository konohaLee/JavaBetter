# 1.自我介绍

面试官您好，我叫李哲，来自山东济南，本科毕业于中国矿业大学计算机科学与技术专业，保研后继续在该学校该专业就读，本硕成绩优异，并多次校级奖学金。在第十六届蓝桥杯算法竞赛中获得江苏省一等奖。

科研方面，我以第一作者身份在人工智能领域国际期刊《Knowledge-Based Systems》（中科院一区TOP，CCF-C类）发表论文一篇，中文译名基于高置信度正负样本对选择的三重对比式图聚类网络，题为《Triple-View Graph Clustering Network Based on High-Confidence Contrastive Learning Strategy》，研究了多视角图对比学习在聚类任务中的应用。

技术上，我专注于 Java 后端开发，熟练掌握 Spring Boot、MyBatis 等主流框架，具备扎实的系统设计和接口开发能力。在高并发场景下有实战经验：曾独立开发“仿淘宝神券系统”，综合运用 Redis + Lua 实现库存原子扣减、RocketMQ 异步解耦、ShardingSphere 分库分表、布隆过滤器防穿透等技术，并通过设计模式和幂等机制提升系统可维护性与稳定性。

2025年暑期，我在途牛旅游网担任后端开发实习生，参与建行定制游和移动疗休等核心业务开发，完成了用户体系对接、订单链路闭环、接口重构及横向越权漏洞修复等工作，积累了微服务架构下的工程实践经验。


# 2.实习内容

## 中国移动疗休：

> 为支持移动疗休业务的功能扩展，升级订单同步接口

### 重构同步接口：

重构酒店、火车、飞机、用车四大核心品类的订单推送逻辑，完成移动新增字段的推送功能;

1. 业务背景：移动方面需要升级订单详情接口，升级后的订单详情需要补充大量的字段，我负责对接这部分内容
    
2. 主要工作：一个是根据移动提供的接口文档，梳理字段，去找缺少的字段从哪里可以获得，然后获取这些信息；另一个是重构推送接口所推送的内容，确保我们推送的信息能够符合移动方的需求
    
3. 工作1:移动缺少的是这个订单所属的品类的详细信息，一共有四大类：火车票；机票；酒店；用车，这四大类在我们公司所属于不同的系统，公司有一个整合的系统叫pga，这些下属的不同品类的详细信息可以通过pga完成透传，这样我们就能通过订单ID获取到一个json对象，再通过解析这个json对象就可以拿到不同品类的信息；
    
4. 接下来就是比较复杂的编码工作了，这部分难度不大但是整体结构十分繁杂，酒店要推送的对象比较简单，大部分信息都在非集合变量中，有关房间具体信息，住户具体信息的作为集合变量；火车票需要将整个订单中的所有票打包成集合（比如两个人从a到b到c，有四张票，这个订单集合就有四个元素），除此之外，改签和退票信息也是类似的集合形式；飞机票类似火车票；用车分成三个，分别是接送机，租车，包车
    
5. 最后是和移动进行联调上线
    

### 修复安全漏洞：

针对查看出团通知书接口，通过RPC调用用户服务进行身份校验，成功修复了横向越权安全漏洞，保障了用户数据的隐私与安全;

1. 首先介绍这个接口的背景：这是我实习的第一个需求，我接到的需求是根据用户的订单id返回这个用户的出团通知书，起初我认为这个工作比较简单，然后开始了接口文档和技术方案的撰写，入参是一个订单ID，出参一个统一封装的response，将String的URL作为message，进行了开发
    
2. 但是mentor检查是提出我没有注意到会出现的横向越权问题，这个问题是如果我只以订单ID做为参数，去查询，那么就会出现一种情况，另一个用户输入了错误的订单ID，那么他就能查到那个ID对应的出团通知书，所以，需要多加一步判断：于是我就加了一步判断：确认当前用户是订单对应的用户才往下获取URL，
    
3. 最后整体流程如下：获取订单ID，通过调用远程的下游服务获取到该订单的userID，并从登录态中拿到当前登录用户的id，判断一致后远程调用下游服务的接口获取该订单对应的URL
    

## 建行定制游相关

业务背景：该项目属于建行疗休，由于我们提供的产品没发满足建行用户的个性化需求，因此选择开发定制游

### 整合用户体系：

实现建行用户静默注册与身份绑定逻辑，通过外部ID与手机号，无缝对接第三方与公司内部会员体系;

定制产品打包与预定单下单：通过RPC依次调用动态打包产品、订单中心等多个下游服务，将建行员工提交的个性化需求（行程、城市、日期等）实时转化为系统内的产品，并生成预定订单；

建行定制游部分主要分三部分：2.18 定制游需求提交，2.19 供应商定制产品推送， 2.17 供应商额外收款

建行有自己的善融app，如果选择我们的话会走我们的定制游逻辑，根据2.18，首先，建行用户提交过来的都是在建行的信息，封装在一个dto中，用户来了，我们首先要为该用户进行静默注册，将建行用户转化为我们的用户后才能进行后续的操作

1. 静默注册：静默注册是在后台完成注册，不会显示的打扰到用户，提高用户的使用体验，静默注册这部分我们主要做了：首先构建一个绑定方法的参数请求dto,这个dto中主要需要建行的userId和手机号，以及将建行对应的伙伴码和P值（写死），将他们写入这个dto中；接着我们用这个请求去远程调用绑定服务，这个服务是专门为第三方用户注册的；绑定成功后，系统后台就有了这个建行用户对应的途牛ID了，这时候我们就可通过建行ID查到途牛ID，这一步依然是通过掉远程接口拿到的，至此，我们就将这个建行用户转化为了途牛用户，并且拿到了ID
    
2. 动态产品打包：动态打包产品，也就是根据用户的自定义需求，实时生成一个动态的产品，首先跟上面一样，构建出一个动态产品的dto，接着，调用远程接口，将这个产品做为参数传入，远程接口返回这个动态产品的动态ID，接着我们构建一个输出的dto，包含产品名称，城市信息，出行信息等内容，这个输出的dto就做为我们获取到的动态产品。
    
3. 调用pga下单：下单部分需要前两步获取的用户ID和产品信息，利用他们构建下单的参数，这个请求中需要包含订单配置信息（用户id，来源类型），备注信息（备注主要是解析产品并转化成备注信息，需求号多少？出发地目的地等），设置下单系统信息，设置渠道信息（建行的渠道相关内容，p值等），最后是子订单信息，子订单可以视作我们习惯上看到的订单；构建好订单信息后调用下单接口完成下单返回结果，此时我们就拿到了这个定制游的订单id，接着在需求表中记录这条需求，这样这个接口的任务就完成了
    
### 业务数据持久化：

设计建行员工出游需求表以持久化关键数据，通过引入version字段实现乐观锁，有效保证了需求单状态更新的一致性，确保了链路的可追溯性；并开发了产品信息回推接口，将针对用户定制的产品同步至建行，最终完成了与第三方渠道的完整业务闭环；

产品回推接口即2.19 供应商定制产品推送：这部分的场景为：我们的客服跟单完成后，我们需要将完成的信息落库，并且将产品信息推送给建行：

1. 入参数为订单ID，客服ID，用户ID，首先我们需要根据订单ID拿到需求单ID，更新需求单ID的客服部分
    
2. 根据需求单的内容，构建一个建行产品，并且写入我们系统的产品库（区分之前的动态产品库）
    
3. 根据建行的文档，构建符合文档要求的output对象，并且通过远程调用的形式推送给建行


## 问：实习的困难和挑战
1. 逐步适应真实的开发流程：在学校，我可能写完代码、功能通过就结束了。但在实习中，我必须学习一套完整的流程：从接收需求、撰写技术方案、参加方案评审、编码实现，到最后的联调上线。
2. 需要考虑到实际场景深入分析业务：还是以那个安全漏洞为例，我最初的分析是“输入一个订单ID，返回一个URL”，这是一个纯技术视角。在被指出“横向越权”风险后
3. 如何与性格不同的人进行团队合作：多问多沟通，不要一个人闷着头做


## 问：这个 `pga` 系统具体扮演了什么角色？它是一个API网关，还是一个服务聚合层？
`pga` 在这里的角色更像是一个**服务聚合层（Service Aggregation Layer）**。它封装了对下游多个垂直业务系统（如酒店、火车票系统）的调用细节。我们上游的业务服务（疗休服务）不需要知道下游各个系统的具体接口地址或协议，我们只需要按照`pga`定义的规范，传入一个订单ID，`pga` 就会去“透传”这个请求，调用对应的下游系统，拿到详情，然后返回一个聚合后的JSON对象。

**为什么不让前端传`userId`？** 绝对不行。安全设计的核心原则是**“永不信任客户端”**。如果前端同时传递 `orderId` 和 `userId`，攻击者可以轻易地伪造请求，用自己的 `userId`（A）去匹配别人的 `orderId`（B）。这种校验毫无意义。

## 问：新增的RPC调用（`orderId` -> `userId`）是否会对接口性能产生明显影响？有没有考虑过其他实现方式，

**为什么后端RPC是最好的？**

1. **数据权威性：** 订单（`orderId`）和用户（`userId`）的归属关系，**唯一权威的来源**是订单服务。我的服务（疗休服务）必须向订单服务查询，才能确认“这个`orderId`到底是不是属于你这个登录用户”。
    
2. **职责分离：** 当前登录用户的`userId`从登录态（如Session或Token）中获取，这是可信的。`orderId`从请求参数中获取，这是不可信的。用可信的数据（登录态`userId`）去校验不可信数据（`orderId`）的归属权，这个逻辑必须在后端完成。
    

## 问：服务编排使用分布式事务相关内容

这个流程涉及多个外部RPC调用和本地DB操作，是一个典型的**分布式事务**场景。

在当前的代码实现中，我们并没有采用重量级的分布式事务方案（如Seata或2PC/3PC），因为这会显著增加系统复杂度和性能开销。我们的处理策略是**“最终一致性 + 补偿机制”**。

1. **代码中的处理：** 如你所见，代码`handleNewDemandWithOrder`中使用的是`try-catch`块。如果在`createPgaOrder`成功后，`saveOrUpdateTourDemandRecord`失败了，`catch`块会捕获异常，记录详细的错误日志（包含`demandId`、`orderId`等关键信息），并向上层返回一个明确的失败响应。
    
2. **补偿机制（核心）：**
    
    - **人工/自动补偿（上游）**：我们依赖`try-catch`中捕获的**详细错误日志**。这些日志会接入公司的监控告警平台。一旦出现“PGA下单成功但本地保存需求失败”的特定错误，会触发告警，由SRE或开发人员介入，分析日志并手动触发补偿流程（比如拿日志中的`orderId`重新调用一次保存接口）。
        

## 乐观锁

# 3.项目经历
## 项目中的难点与挑战是什么
主要在于学习和尝试使用新的中间件来优化现有的内容，尝试将设计模式的思想融入开发中：“仿淘宝神券系统” 中，要支撑大量用户同时秒杀领券，必须解决高并发下的性能、数据一致性以及代码可扩展性的问题。

在寻求改进的过程中，学习和利用到了很多之前没有接触过的技术和设计模式。最终也是基于 **Redisson**、**Lua 脚本**、**RabbitMQ**（或 RocketMQ）等中间件，以及**责任链**和**模板方法**等设计模式，完成了一个整体的优化。

具体来说：
- **学习中间件应用：**
    - 用 **Redisson** 的分布式锁来设计自定义注解解决幂等性问题和重复提交（它解决了可重入、可重试、超时释放、主从一致性四个问题）。
    - 学习使用canal中间件来监听binlog，以实现缓存一致性处理。
    - 学习使用**RocketMQ**执行异步操作，将同步写库操作解耦，以此来削峰填谷，极大提高了秒杀接口的性能。
- **设计模式应用：**
    - 应用**责任链设计模式**来分层校验优惠券模板参数，取代了复杂的 `if-else`，提升了系统的拓展性。
    - 运用**模板方法设计模式**重构了消息队列的发送功能，将固定的发送流程和易变的配置逻辑分离，增强了代码复用性。
## excel分发与延迟队列兜底

优惠券分发要分发给有需要的人，这部分业务就是通过excel记录需要被发送的人，解析出他们并且发送

**具体的推送逻辑是**：获取到用户信息的excel后，将优惠券写入这些用户的领券列表中，并且根据情况选择是否通知用户

### excel中的数据怎么来的？

1.数据开发工程师一般会通过用户的点击，浏览时长等信息构建用户的画像，用于识别某个商品的潜在用户，根据画像，平台运营人员可以有针对的进行优惠券的投放工作，比如推送个消息给用户说可以领个xxx券
运营如果要投放一部分针对商品的优惠券，会向平台提出，平台会给出一组符合投放条件的用户信息列表，这个列表可以使用excel存储，为什么使用excel，首先目前也有很多企业采用这种方案，其次我们没有接入用户画像平台，所以使用这个来实现

### 数据库表的设计

设计了优惠券分发任务表，因为这种优惠券属于资格券，不会分发很多，所以我们没有进行分库分表

关于这个虚构Excel表的生成，我们使用了faker开源库来伪造了一部分数据（excel中有用户id，手机号，邮箱）实际应用为了不泄露隐私，我们可能只能拿到用户id，剩余的内容通过用户信息服务查询获得

### EasyExcel

这是一个Java上处理Excel的工具，我们使用faker生成了具有一百万数据的Excel表（30m左右），它适合处理大规模excel数据，相比于hutool，在处理这个文件的时候（获取行数），它的内存占用是hutool的十分之一

解析Excel行数的目的：预览任务规模；任务完成后与行数进行对比看是否有遗漏

### 线程池异步优化Excel

百万数据量的解析耗时5秒，这种情况无法接受，我们选择使用redisson延迟队列和线程池来对这个人物进行优化，提交Excel解析请求后，后管系统会提交到线程池，并向redisson延迟队列发送一个20秒的延时任务，线程池任务是利用EE解析行数，解析完成后返回数据并且更新数据库-优惠券分发任务表中的send_num字段，延迟队列20秒后触发，检查是否有send_num，如果有说明成功，没有就重试解析

以上延迟队列能干到的事情使用rocket消息队列都能干到，但是实际开发中不是所有的公司都会用rocket，所以尝试了不同的方案

### 线程池的使用

可能习惯使用 `Executors` 工具类直接创建线程池，这种是不推荐的。虽然 `Executors` 提供了创建线程池的便捷方法，然而，`Executors` 基于默认配置创建的线程池可能并不适合所有场景，这里我们说下每个方法创建的线程池都有哪些弊端：

- `newFixedThreadPool` 和 `newSingleThreadExecutor`：这两种固定大小的线程池使用无界的 `LinkedBlockingQueue` 作为工作队列。当任务提交速度超过处理速度时，工作队列会不断增长，可能导致内存溢出。

- `newScheduledThreadPool`：虽然最大线程数是 `Integer` 最大值，但是因为阻塞队列是无界的，所以核心问题同上。

- `newCachedThreadPool`：核心线程数为 0，使用同步的 `SynchronousQueue`，并且允许创建无限数量的线程。在高并发情况下，可能会创建大量线程，导致系统资源耗尽，甚至使系统崩溃。

1. 核心线程数：后管任务不会很频繁，直接CPU核心数
2. 最大线程数：属于IO密集，CPU✖️2
3. 工作队列：理论上说我们不会有阻塞的情况，因为设置的线程数不少，所以如果使用不存储任务的同步队列。
4. 拒绝策略：有兜底，直接丢弃

private final ExecutorService executorService = new ThreadPoolExecutor(  
        Runtime.getRuntime().availableProcessors(),  
        Runtime.getRuntime().availableProcessors() << 1,  
        60,  
        TimeUnit.SECONDS,  
        new SynchronousQueue<>(),  
        new ThreadPoolExecutor.DiscardPolicy()  
);

### redisson延迟队列兜底

在提交任务到线程池后，我们会将这个任务也提交到redisson的延迟队列，20秒后该消息会从延迟队列转移到阻塞队列，此时阻塞队列的消费者一直监听阻塞队列，使用.take()方法取出

生产者：延迟队列调用offer方法

消费者：实现了commandlinerunner接口的内部类启动后自动运行监听程序
## 为什么选择“线程池 + Redisson延迟队列”这个组合，而不是一个更统一的方案？

Controller发MQ，Consumer做解析”是这个场景的**标准解**。它职责清晰，并且能利用MQ的可靠投递、重试和死信队列来保证任务“至少执行一次”，这比我自己实现的`ThreadLocal` + `Redisson`兜底要健壮得多。

当时没有采用这个方案，主要是出于两点考虑：

1. **技术栈探索：** 正如文档里提到的，我们想尝试`Redisson`延迟队列的用法，探索在没有`RocketMQ`（或不想为这种“非核心”业务引入重量级MQ）的场景下，如何用Redis实现一个轻量级的异步任务兜底。
2. **业务场景：** 这个“Excel分发”在我们的设定中是一个**后台运营功能**，并发量很低，而且对实时性要求不高（5秒或30秒完成都可以接受）。这使得我们有空间去“牺牲”一点健壮性来尝试不同的技术实现。
## 怎么处理缓存击穿的
缓存击穿了那么访问的就要去重建缓存，如何协调这些重建缓存的线程是一个大问题，主要思路就是让重建缓存的线程拿锁，其他重建线程拿不到，最常见的是逻辑过期
### 预热
- **预热**：预热是指在活动开始之前，针对特定的热点数据，将他们从数据库加载到缓存中，避免海量请求第一次访问数据库而不访问缓存造成的冲击
### 永不过期
永不过期是指在活动开始之前，设置过期时间为-1，在存入缓存时，我们存一个包含“逻辑过期时间”的 JSON 对象。缺点是这些数据一直在内存中
**访问流程**：
- **线程A** `get(key)`，拿到了上面的 JSON。
- **检查 `logicalExpireTime`**：
    - **case 1：未过期**。直接返回 `data`。（这是 99% 的情况）
    - **case 2：已过期**
        - **A 线程**：**立即返回“旧的”`data`**。
        - **同时**，A 线程尝试获取一个**互斥锁**。
        - **如果 A 获取锁成功**：A 开启一个**异步线程**（或使用线程池）去**重建缓存**（`get(db)` -> `set(cache, new_data, new_logical_time)`）。
        - **如果 A 获取锁失败**：说明已有其他线程在重建了，A 直接返回旧数据即可。
### 双重判定锁
使用分布式锁，确保只有一个获取不到缓存的线程去数据库查数据并重建缓存：
多个线程访问，首先第一次判断是否有缓存，有的话直接返回缓存，没有的话去获取锁。**在获取到分布式锁之后，再次查询一次缓存是否存在**。如果缓存中存在数据，说明其他线程重建过了，就直接返回；如果不存在，那由这个线程重建数据库。这样就可以避免大量请求访问数据库。
- ***对数据一致性要求非常高，能容忍短时间性能下降（线程等待）的场景，使用方案一：双重判定锁***
- ***对系统可用性要求极高，能容忍短时间数据不一致（读到旧数据）的场景，使用方案二：逻辑过期***
## 怎么保持缓存一致性的？cas和延迟双删（项目中没用）
删除缓存是一步很快的操作，而更新数据库比较慢，如果先删除缓存，那么在更新数据库的过程中会有其他线程读到未更新的数据库数据，并且利用这个数据重建缓存的情况出现，所以一个简单的方案就是先更新数据库，再删除缓存（cacheaside）。但是这也会造成以下情况
- 线程A更新数据库 → 删除缓存。
- 线程B在A删除缓存前读取旧缓存，导致B后续请求读到脏数据，不过这种方式对于更新数据库到删除缓存之间访问的线程来说会读到脏数据，但是对并发不高的业务来说无伤大雅。如果实在忍受不了这点一致性，直接用canal这种强一致性的
拓展到延迟双删，他是解决先删除缓存再更新数据库这种操作带来的脏数据覆盖缓存的问题的
1. 先删除缓存：在更新数据库之前，首先删除缓存中的数据，以避免在更新过程中读取到旧数据。
2. 更新数据库：执行数据库的更新操作。
3. 休眠一段时间：根据业务读取数据的平均耗时，设置一个休眠时间（如1秒），以确保在此期间内所有可能的读请求都已经结束，并且可能因读取旧数据而写入的脏数据已经被缓存。
4. 再次删除缓存：休眠结束后，再次删除缓存中的数据，以清除可能因读请求而写入的脏数据。
这个延迟到底是多久呢？ 这个休眠一会，一般多久呢？十几毫秒？几百毫秒？还是1秒？

>这个休眠时间 = 读业务逻辑数据的耗时 + 几百毫秒。它为了确保读请求结束，写请求可以删除读请求可能带来的缓存脏数据。

这种方案还算可以，只有休眠那一会（比如就那1秒），可能有脏数据，一般业务也会接受的。
[[牛券-秒杀-canal#^fd8815]]秒杀部分使用canal同步插入的用户领的券和缓存
## 项目中使用的redis缓存淘汰策略
如果资源足够的话，会选择不淘汰任何数据，如果你的业务数据的访问比较平均，不存在明显的冷热区别，那么 LRU 可以满足一般的使用需求。如果你的业务具备很强的时效性，而且是存在大促商品这种明显的热点数据，那么推荐你使用 LFU。

因为牛券里的一些模板也不全是常用的，适当内存吃紧的时候可以淘汰一部分带过期时间的 Key，再加上牛券中的优惠券存在明显的冷热数据现象，使用 `volatile-lfu`，可以有效避免 LRU 的缓存污染问题。

## 缓存预热
因为优惠券模板（尤其是秒杀券）的创建量非常大，并且很多优惠券的生效时间是在遥远的未来（比如一个月后）。如果我们采取系统启动时全量加载，或者创建时就全部写入缓存的策略，那会浪费大量宝贵的 Redis 内存。

所以，我们设计的是一套**“分层异步入库 + 定时分批预热”**的精细化策略。

**首先，在创建阶段，我们就做了分层处理和异步解耦：**

1. 当商家在后台创建优惠券时，我们的服务会获取优惠券的生效时间。
2. 我们判断，如果这个优惠券的生效时间是在**7天以后**，我们就认为它是“冷数据”。
3. 对于这些“冷数据”，我们并**不会同步写入 MySQL**，因为这会拖慢创建接口的响应速度。相反，我们会将这个优惠券模板的完整信息封装成一条消息，发送到 **RocketMQ** 消息队列中。
4. 然后，由下游的一个专门的消费者服务来监听这个 Topic，异步地将这条数据持久化到 MySQL 数据库中。

这个 RocketMQ 的解耦 操作，就是为了**提高创建优惠券这个接口的高并发**能力，让商家前端的体验更好。

**其次，在预热阶段，我们通过定时任务来主动加载：**

1. 我们系统里有一个定时任务（比如使用 `xxl-job`），它会每天凌晨定时触发。
2. 这个任务的核心逻辑，就是去扫描 MySQL 数据库，专门查询**在未来3天内即将开始生效**的优惠券模板。
3. 查询到这些“即将变热”的数据后，任务会批量地将它们加载到 Redis 缓存中。

**在写入缓存的实现细节上，我们还做了两个关键优化：**

1. **数据结构：** 我们使用的是 **Redis Hash** 结构，而不是 String。这是因为我们需要在 Hash 结构里单独存放 `stock`（库存）字段，以便在后续秒杀时，可以利用 `HINCRBY` 命令对库存进行**原子扣减**。
    
2. **写入原子性：** 在写入缓存时，我们既要写入数据（`HMSET`），也要设置过期时间（`EXPIREAT`，设置为优惠券的 `valid_end_time`）。为了防止这两个操作之间发生宕机导致数据永不过期，我们使用了 **LUA 脚本**将这两个命令打包成一个原子操作来执行，确保了数据和过期时间的一致性。
    

这套策略的好处是，RocketMQ 的异步化 保证了创建接口的高吞吐；而“定时预热3天内数据”的方案，则确保了 Redis 中只存放了近期活跃的优惠券，在**内存成本**和**缓存命中率**之间做到了一个很好的平衡，完美避免了活动开始瞬间的冷启动和缓存击穿问题。
## 布隆过滤器怎么使用，它的参数（数组长度，哈希函数的个数）要如何设置？会误判吗？

为了防止缓存穿透，我使用了 **布隆过滤器（Bloom Filter）** 来拦截大量请求中对数据库中不存在的数据访问。它实际上是一个很长的二进制向量和一系列随机映射函数。布隆过滤器可以用于检索一个元素是否在一个集合中。它的优点是空间效率和查询时间都比一般的算法要好的多，缺点是有一定的误识别率和删除困难。可以快速判断一个数据“可能存在”或“一定不存在”。
// 初始化参数：预期元素数量640，误判率0.1%
使用上，我采用的是 **Guava 提供的 BloomFilter** 类库。**预计插入数量（n）**：预计将要加入布隆过滤器的数据条数，我预估的是 1w；**误判率（f）**：控制误判概率，1% 是一个常用且折中选择；底层会自动根据这两个参数计算出最优的位数组长度（m）和哈希函数个数（k），我们不需要手动设定；
1. 元素数量640：

> - 我们系统预计单个商户平台的优惠券模板数量在500左右
> - 预留20%的buffer空间，所以配置为640
> - 如果超过这个数量，布隆过滤器会自动扩容，但误判率会上升
> - 2. 误判率0.1%：
> - 误判率越低，需要的内存越大
> - 0.1%意味着1000次查询中可能有1次误判（判断存在但实际不存在）
> - 我们有空值缓存兜底，即使误判也不会击穿到数据库
> - 根据公式计算，640个元素、0.1%误判率大约需要 7KB内存，成本极低
> 3. 为什么不设置更低的误判率？
 0.01%的误判率需要的内存是0.1%的10倍。我们有多层防护（布隆过滤器+空值缓存+双重检查锁），0.1%已经足够

### 是否会误判？
会，但只会出现“误判存在”，不会出现“误判不存在”。这意味着：布隆过滤器可能说某个 key 存在，但其实它没有，但不会把一个真的 key 给错判为不存在。因此，我们配合 Redis 和 DB 再做一次兜底判断，整体上非常安全可靠。
### 扩容
可以创建一个容量为原先 1.5 倍或 2 倍 的布隆过滤器（若业务侧能够预估规模，则以业务评估为准），然后从数据库中读取数据并重新加载到新的布隆过滤器中即可。

## 你用雪花算法生成全局唯一id，为什么要使用全局唯一id，雪花算法的优缺点有哪些？
在分布式系统中，我们需要为业务数据生成**唯一且有序的ID**，比如用户ID、订单号等。使用数据库自增ID会有性能瓶颈和单点问题，因此我采用了**雪花算法（Snowflake）**来生成全局唯一ID。
### 使用雪花算法的好处：
- **高性能**：在内存中生成，吞吐量高；
- **全局唯一**：结合时间戳+机器标识+序列号，避免重复；
- **趋势递增**：便于按时间排序，提高写入性能；
- **不依赖数据库**：适用于分布式系统、微服务架构
### 缺点或风险：
- **依赖系统时间**：如果服务器时间回拨，可能会导致ID重复或系统阻塞；
- **ID较长**：占用long型（64位），在某些极端存储或传输场景中略大；
- **机器ID配置复杂**：需要保证不同服务实例间机器ID不重复，否则可能会冲突。
我在项目中使用自己封装的 snowflake 工具类实现 ID 生成，有效保障了系统的高并发写入场景下 ID 唯一性与有序性。


